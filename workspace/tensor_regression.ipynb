{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f9c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aba48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d79f6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]], dtype=np.float32) \n",
    "y_data = np.array([[3], [5], [7], [9], [11]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2be1f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(tf.random.normal([2, 1]), name='weight')   # tf 변수 이름을 weight로 짓겠다는 거임. 아래도 마찬가지 \n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.W) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2505abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    return tf.reduce_mean(tf.square(y_pred - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5285bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, Loss:0.8794978260993958, W: [0.5485629 0.7905137], b: [2.1295009]\n",
      "Step:1000, Loss:0.666898787021637, W: [0.6330525  0.82759356], b: [2.0820837]\n",
      "Step:2000, Loss:0.6152917146682739, W: [0.6724768 0.8097006], b: [2.024768]\n",
      "Step:3000, Loss:0.5676774978637695, W: [0.71023816 0.7923793 ], b: [1.9696813]\n",
      "Step:4000, Loss:0.5237485766410828, W: [0.74650896 0.7757403 ], b: [1.9167703]\n",
      "Step:5000, Loss:0.48321977257728577, W: [0.7813476  0.75975883], b: [1.8659478]\n",
      "Step:6000, Loss:0.44582653045654297, W: [0.8148112 0.7444078], b: [1.8171312]\n",
      "Step:7000, Loss:0.41132673621177673, W: [0.8469543 0.7296627], b: [1.770241]\n",
      "Step:8000, Loss:0.3794974684715271, W: [0.8778281  0.71550035], b: [1.7252027]\n",
      "Step:9000, Loss:0.3501308560371399, W: [0.90748346 0.70189625], b: [1.6819414]\n",
      "Step:10000, Loss:0.3230368494987488, W: [0.93596786 0.68882996], b: [1.6403873]\n",
      "Step:11000, Loss:0.2980394959449768, W: [0.963328  0.6762787], b: [1.6004744]\n",
      "Step:12000, Loss:0.2749764919281006, W: [0.9896082  0.66422355], b: [1.5621363]\n",
      "Step:13000, Loss:0.2536984086036682, W: [1.0148504  0.65264475], b: [1.5253112]\n",
      "Step:14000, Loss:0.23406729102134705, W: [1.0390947 0.6415238], b: [1.4899398]\n",
      "Step:15000, Loss:0.21595481038093567, W: [1.062384  0.6308404], b: [1.4559642]\n",
      "Step:16000, Loss:0.199244886636734, W: [1.0847504 0.6205809], b: [1.4233297]\n",
      "Step:17000, Loss:0.18382702767848969, W: [1.1062379 0.6107253], b: [1.3919835]\n",
      "Step:18000, Loss:0.16960251331329346, W: [1.1268744  0.60125864], b: [1.3618735]\n",
      "Step:19000, Loss:0.1564791202545166, W: [1.1466962 0.5921658], b: [1.3329538]\n",
      "Step:20000, Loss:0.1443706601858139, W: [1.1657381 0.5834322], b: [1.3051741]\n",
      "Step:21000, Loss:0.1331998109817505, W: [1.184025  0.5750453], b: [1.2784914]\n",
      "Step:22000, Loss:0.12289305031299591, W: [1.2015922 0.5669859], b: [1.252862]\n",
      "Step:23000, Loss:0.11338397115468979, W: [1.2184637 0.5592489], b: [1.2282432]\n",
      "Step:24000, Loss:0.10461103916168213, W: [1.2346693 0.5518151], b: [1.2045966]\n",
      "Step:25000, Loss:0.09651664644479752, W: [1.2502357 0.544675 ], b: [1.181884]\n",
      "Step:26000, Loss:0.0890490934252739, W: [1.2651856  0.53781915], b: [1.1600667]\n",
      "Step:27000, Loss:0.08215869218111038, W: [1.279549  0.5312304], b: [1.1391115]\n",
      "Step:28000, Loss:0.07580159604549408, W: [1.2933431 0.5249063], b: [1.118981]\n",
      "Step:29000, Loss:0.06993715465068817, W: [1.3065912 0.5188286], b: [1.0996487]\n",
      "Step:30000, Loss:0.06452560424804688, W: [1.3193206 0.5129918], b: [1.0810769]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.0001)\n",
    "\n",
    "for epoch in range(5001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, x_data, y_data)\n",
    "        gradients = tape.gradient(loss, [model.W, model.b])\n",
    "        optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Step:{epoch}, Loss:{loss.numpy()}, W: {model.W.numpy().flatten()}, b: {model.b.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc6f365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.587942 14.420255 16.252567]\n"
     ]
    }
   ],
   "source": [
    "pred = model(np.array([[6, 7], [7, 8], [8, 9]], dtype=np.float32))\n",
    "print(pred.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4022197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/data-01-test-score.csv', header=None)\n",
    "\n",
    "x_frame = df.values[:, :-1].astype('float32')\n",
    "y_frame = df.values[:, -1].reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11011786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(tf.random.normal([3, 1]), name='weight')\n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.W) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b396fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, Loss:127284.1015625, W: [-0.13746428 -1.3773553   0.8311126 ], b: [0.11668225]\n",
      "Step:100, Loss:35.61631774902344, W: [ 0.738077   -0.44827852  1.7105292 ], b: [0.12729485]\n",
      "Step:200, Loss:33.23002624511719, W: [ 0.7294971 -0.4099949  1.6814607], b: [0.12695979]\n",
      "Step:300, Loss:31.040771484375, W: [ 0.7210918  -0.37327084  1.6537464 ], b: [0.12663294]\n",
      "Step:400, Loss:29.032024383544922, W: [ 0.7128587  -0.33804137  1.6273252 ], b: [0.12631394]\n",
      "Step:500, Loss:27.188806533813477, W: [ 0.7047947  -0.30424398  1.6021389 ], b: [0.1260024]\n",
      "Step:600, Loss:25.497203826904297, W: [ 0.6968978  -0.27181852  1.5781307 ], b: [0.12569803]\n",
      "Step:700, Loss:23.944597244262695, W: [ 0.6891655  -0.24070781  1.5552474 ], b: [0.12540038]\n",
      "Step:800, Loss:22.519390106201172, W: [ 0.6815942  -0.21085748  1.5334392 ], b: [0.12510927]\n",
      "Step:900, Loss:21.211040496826172, W: [ 0.6741813  -0.18221469  1.5126569 ], b: [0.12482429]\n",
      "Step:1000, Loss:20.009733200073242, W: [ 0.6669241  -0.15472946  1.4928544 ], b: [0.12454522]\n",
      "Step:1100, Loss:18.906639099121094, W: [ 0.65982056 -0.12835328  1.4739864 ], b: [0.12427171]\n",
      "Step:1200, Loss:17.89354705810547, W: [ 0.65286744 -0.1030401   1.4560108 ], b: [0.12400354]\n",
      "Step:1300, Loss:16.96300506591797, W: [ 0.6460619  -0.07874588  1.4388875 ], b: [0.12374046]\n",
      "Step:1400, Loss:16.10817527770996, W: [ 0.63940185 -0.05542825  1.422577  ], b: [0.12348219]\n",
      "Step:1500, Loss:15.322779655456543, W: [ 0.6328844  -0.03304689  1.407043  ], b: [0.12322854]\n",
      "Step:1600, Loss:14.601055145263672, W: [ 0.62650716 -0.01156292  1.3922495 ], b: [0.12297928]\n",
      "Step:1700, Loss:13.937766075134277, W: [0.6202675  0.00906102 1.3781626 ], b: [0.12273418]\n",
      "Step:1800, Loss:13.328068733215332, W: [0.6141627  0.02886039 1.3647504 ], b: [0.12249306]\n",
      "Step:1900, Loss:12.76754379272461, W: [0.60819036 0.04786908 1.3519821 ], b: [0.12225569]\n",
      "Step:2000, Loss:12.252140998840332, W: [0.602348   0.06611995 1.3398279 ], b: [0.12202193]\n",
      "Step:2100, Loss:11.77812385559082, W: [0.5966336  0.08364422 1.3282595 ], b: [0.1217916]\n",
      "Step:2200, Loss:11.342133522033691, W: [0.5910441  0.10047179 1.3172504 ], b: [0.12156451]\n",
      "Step:2300, Loss:10.941021919250488, W: [0.5855775  0.11663144 1.3067745 ], b: [0.12134051]\n",
      "Step:2400, Loss:10.571907043457031, W: [0.5802312  0.13215041 1.2968078 ], b: [0.12111947]\n",
      "Step:2500, Loss:10.232220649719238, W: [0.5750029  0.14705512 1.2873265 ], b: [0.12090123]\n",
      "Step:2600, Loss:9.919539451599121, W: [0.5698906  0.16137105 1.2783079 ], b: [0.12068562]\n",
      "Step:2700, Loss:9.63162899017334, W: [0.5648914 0.1751219 1.2697315], b: [0.12047257]\n",
      "Step:2800, Loss:9.366497039794922, W: [0.5600034  0.18833122 1.2615763 ], b: [0.12026194]\n",
      "Step:2900, Loss:9.122286796569824, W: [0.5552245 0.201021  1.2538227], b: [0.12005358]\n",
      "Step:3000, Loss:8.8972806930542, W: [0.5505523  0.21321245 1.2464523 ], b: [0.11984739]\n",
      "Step:3100, Loss:8.689932823181152, W: [0.54598475 0.22492602 1.2394472 ], b: [0.11964327]\n",
      "Step:3200, Loss:8.49884033203125, W: [0.54151994 0.2361808  1.2327906 ], b: [0.11944113]\n",
      "Step:3300, Loss:8.322643280029297, W: [0.53715545 0.24699621 1.2264657 ], b: [0.11924081]\n",
      "Step:3400, Loss:8.160148620605469, W: [0.5328894  0.25738955 1.2204577 ], b: [0.11904228]\n",
      "Step:3500, Loss:8.010275840759277, W: [0.5287198  0.26737836 1.2147511 ], b: [0.11884544]\n",
      "Step:3600, Loss:7.872011184692383, W: [0.52464443 0.27697864 1.2093328 ], b: [0.11865018]\n",
      "Step:3700, Loss:7.744390964508057, W: [0.5206615  0.28620654 1.2041885 ], b: [0.11845644]\n",
      "Step:3800, Loss:7.626593112945557, W: [0.51676905 0.29507688 1.1993054 ], b: [0.11826412]\n",
      "Step:3900, Loss:7.5178070068359375, W: [0.5129651  0.30360427 1.1946715 ], b: [0.11807314]\n",
      "Step:4000, Loss:7.417329788208008, W: [0.5092478  0.31180254 1.1902752 ], b: [0.1178835]\n",
      "Step:4100, Loss:7.3245134353637695, W: [0.5056156 0.3196849 1.1861047], b: [0.11769509]\n",
      "Step:4200, Loss:7.238715648651123, W: [0.50206614 0.32726434 1.1821499 ], b: [0.11750779]\n",
      "Step:4300, Loss:7.159430503845215, W: [0.49859846 0.33455276 1.1783999 ], b: [0.11732165]\n",
      "Step:4400, Loss:7.086085796356201, W: [0.49521014 0.34156194 1.1748457 ], b: [0.11713651]\n",
      "Step:4500, Loss:7.018251895904541, W: [0.4918998  0.34830308 1.1714776 ], b: [0.11695231]\n",
      "Step:4600, Loss:6.955499172210693, W: [0.4886656  0.35478717 1.1682866 ], b: [0.11676913]\n",
      "Step:4700, Loss:6.897400379180908, W: [0.48550597 0.36102414 1.1652647 ], b: [0.11658683]\n",
      "Step:4800, Loss:6.843608379364014, W: [0.4824194  0.36702433 1.1624031 ], b: [0.11640534]\n",
      "Step:4900, Loss:6.793793201446533, W: [0.4794041  0.37279654 1.1596953 ], b: [0.11622464]\n",
      "Step:5000, Loss:6.747639179229736, W: [0.4764588  0.37835047 1.1571325 ], b: [0.11604469]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.00001)\n",
    "\n",
    "for epoch in range(5001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, x_frame, y_frame)\n",
    "        gradients = tape.gradient(loss, [model.W, model.b])\n",
    "        optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Step:{epoch}, Loss:{loss.numpy()}, W: {model.W.numpy().flatten()}, b: {model.b.numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
